{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6bbd17-300b-4c70-af5e-877681b23e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv genai-fake-article\n",
    "!source genai-fake-article/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef810291-30f6-41d4-8070-9643c67d35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8c0bd9-7abc-4796-b8c7-3b03815e1e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure created successfully!\n"
     ]
    }
   ],
   "source": [
    "# create folder/files structure\n",
    "\n",
    "import os\n",
    "\n",
    "folders = [\n",
    "    'genai-fake-article/data',\n",
    "    'genai-fake-article/models',\n",
    "    'genai-fake-article/scripts',\n",
    "    'genai-fake-article/outputs',\n",
    "]\n",
    "\n",
    "files = [\n",
    "    'genai-fake-article/README.md',\n",
    "    'genai-fake-article/data/arxiv_papers.json',\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for file_path in files:\n",
    "    open(file_path, 'w').close()\n",
    "\n",
    "print(\"Structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481affbc-8662-46d5-b767-865f164eeb5c",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e5416d-5c7f-424c-9c3c-ff0c64745c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to genai-fake-article/data/arxiv_papers.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=100):\n",
    "    # Define the base URL and parameters\n",
    "    base_url = 'http://export.arxiv.org/api/query'\n",
    "    params = {\n",
    "        'search_query': query,\n",
    "        'max_results': max_results,\n",
    "        'start': 0,\n",
    "        'sortBy': 'relevance',\n",
    "        'sortOrder': 'descending'\n",
    "    }\n",
    "    \n",
    "    # Make the request to arXiv API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from arXiv API, status code: {response.status_code}\")\n",
    "    \n",
    "    # Parse the XML response\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    # Extract titles and abstracts\n",
    "    papers = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
    "        abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
    "        papers.append({'title': title, 'abstract': abstract})\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def save_papers_to_json(papers, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(papers, f, indent=4)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Tests\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"machine learning\"\n",
    "    max_results = 10 \n",
    "    papers = fetch_arxiv_papers(query, max_results)\n",
    "    save_papers_to_json(papers, 'genai-fake-article/data/arxiv_papers.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfef15-d30e-4a77-8c91-336c2e1b08ba",
   "metadata": {},
   "source": [
    "### Generate fake titles and abstracts based on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f290e0-ef5b-4d61-931b-09f700d63cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated papers saved to genai-fake-article/outputs/fake_papers.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "def load_titles(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        papers = json.load(f)\n",
    "    return [paper['title'] for paper in papers]\n",
    "\n",
    "def generate_abstracts(titles, model_name='gpt2', max_length=200):\n",
    "    generator = pipeline('text-generation', model=model_name)\n",
    "    fake_papers = []\n",
    "    for title in titles:\n",
    "        abstract = generator(f\"Title: {title}\\nAbstract:\", max_length=max_length, num_return_sequences=1)[0]['generated_text']\n",
    "        fake_papers.append({'title': title, 'abstract': abstract})\n",
    "    return fake_papers\n",
    "\n",
    "def save_generated_papers(fake_papers, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(fake_papers, f, indent=4)\n",
    "    print(f\"Generated papers saved to {filename}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    titles = load_titles('genai-fake-article/data/arxiv_papers.json')\n",
    "    fake_papers = generate_abstracts(titles)\n",
    "    save_generated_papers(fake_papers, 'genai-fake-article/outputs/fake_papers.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dfb7d-50c3-4d48-936a-c7cc5e507774",
   "metadata": {},
   "source": [
    "### Generate fake articles from specific titles/abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6e3af0-b3ba-40d4-aea8-ee373f6bfe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper saved to genai-fake-article/outputs/Lecture Notes: Optimization for Machine Learning.docx\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "from docx import Document\n",
    "\n",
    "def load_papers(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        papers = json.load(f)\n",
    "    return papers\n",
    "\n",
    "def generate_section(title, abstract, section_name, model_name='gpt2', max_length=300):\n",
    "    generator = pipeline('text-generation', model=model_name)\n",
    "    prompt = f\"Title: {title}\\nAbstract: {abstract}\\n{section_name}:\\n\"\n",
    "    section_text = generator(prompt, max_length=max_length, num_return_sequences=1)[0]['generated_text']\n",
    "    return clean_generated_text(section_text, section_name)\n",
    "\n",
    "def generate_full_paper(paper, sections, model_name='gpt2', max_length=300):\n",
    "    full_paper = {\"title\": paper['title'], \"abstract\": paper['abstract']}\n",
    "    for section in sections:\n",
    "        section_text = generate_section(paper['title'], paper['abstract'], section, model_name, max_length)\n",
    "        full_paper[section] = section_text\n",
    "    return full_paper\n",
    "\n",
    "def clean_generated_text(text, section_name):\n",
    "    \"\"\"\n",
    "    Removes redundant title, abstract, and section title from the generated text.\n",
    "    \"\"\"\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    # Remove lines that start with \"Title:\", \"Abstract:\", or the section name\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        lower_line = line.strip().lower()\n",
    "        if not lower_line.startswith(\"title:\") and \\\n",
    "           not lower_line.startswith(\"abstract:\") and \\\n",
    "           not lower_line.startswith(section_name.lower()):\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = ' '.join(cleaned_lines)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def save_paper_to_docx(paper, filename):\n",
    "    doc = Document()\n",
    "    doc.add_heading(paper['title'], 0)\n",
    "    doc.add_heading('Abstract', level=1)\n",
    "    doc.add_paragraph(paper['abstract'])\n",
    "\n",
    "    for section, content in paper.items():\n",
    "        if section not in ['title', 'abstract']:\n",
    "            doc.add_heading(section, level=1)\n",
    "            doc.add_paragraph(content)\n",
    "    \n",
    "    doc.save(filename)\n",
    "    print(f\"Paper saved to {filename}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load papers (which includes both titles and abstracts)\n",
    "    papers = load_papers('genai-fake-article/data/arxiv_papers.json')\n",
    "    # TODO: Look at the possibility of making this dynamic to process either one or any specified number of papers\n",
    "    paper = papers[0] \n",
    "\n",
    "    # Define the sections of the paper\n",
    "    # TODO: We can possibly make this dynamic as well, look into the logic later\n",
    "    sections = ['Introduction', 'Basic concepts in optimization and analysis', 'Stochastic Gradient Descent', 'Generalization and Non-Smooth Optimization', 'Regularization','Adaptive Regularization', 'Variance Reduction', 'Nesterov Acceleration', 'The conditional gradient method', 'Second order methods for machine learning', 'Hyperparameter Optimization', 'Bibliography']\n",
    "\n",
    "    # Generate the full paper\n",
    "    full_paper = generate_full_paper(paper, sections)\n",
    "\n",
    "    # Save the paper to a .docx file\n",
    "    save_paper_to_docx(full_paper, f'genai-fake-article/outputs/{paper[\"title\"]}.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
